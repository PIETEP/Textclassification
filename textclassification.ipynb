{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier,LogisticRegression\n",
    "from sklearn.pipeline import FeatureUnion,Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDividor:\n",
    "    INDEX_CATEGORY=0\n",
    "    INDEX_ROOT_FORM=6\n",
    "    TARGET_CATEGORIES=[\"名詞\", \"動詞\",  \"形容詞\", \"副詞\", \"連体詞\", \"感動詞\"]\n",
    "    \n",
    "    def __init__(self,dictionary=\"-Ochasen -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd/\"):\n",
    "        self.dictionary=dictionary\n",
    "        self.tagger=MeCab.Tagger(self.dictionary)\n",
    "    \n",
    "    def extract_words(self,text):\n",
    "        if not (text or textone):\n",
    "            return []\n",
    "        \n",
    "        words=[]\n",
    "        \n",
    "        node=self.tagger.parseToNode(text)\n",
    "        while node:\n",
    "            features=node.feature.split(',')\n",
    "            \n",
    "            if features[self.INDEX_CATEGORY] in self.TARGET_CATEGORIES:\n",
    "                if features[self.INDEX_ROOT_FORM]==\"*\":\n",
    "                    words.append(node.surface)\n",
    "                else:\n",
    "                    #prefer root form\n",
    "                    words.append(features[self.INDEX_ROOT_FORM])\n",
    "            node=node.next\n",
    "        \n",
    "        return words\n",
    "\n",
    "class TextExtractor(BaseEstimator,TransformerMixin):\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,rows):\n",
    "        return rows[:,0]\n",
    "    \n",
    "class TextExtractorone(BaseEstimator,TransformerMixin):\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,rows):\n",
    "        return rows[:,1]\n",
    "\n",
    "class TextExtractortwo(BaseEstimator,TransformerMixin):\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,rows):\n",
    "        return rows[:,2]\n",
    "    \n",
    "class OtherFeaturesExtractor(BaseEstimator,TransformerMixin):\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,rows):\n",
    "        return rows[:,1:].astype('float')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using mutiple text column\n",
    "\n",
    "train=pd.read_csv('./data/train.csv')\n",
    "test=pd.read_csv('./data/test.csv')\n",
    "    \n",
    "train_data=train.drop(\"\"\"特定の列カラム\"\"\",axis=1)\n",
    "train_data=train_data.drop(\"\"\"特定の列カラム\"\"\",axis=1)\n",
    "\n",
    "train_label=train[\"\"\"特定の列カラム\"\"\"]\n",
    "    \n",
    "test_data=test.drop(\"\"\"\"特定の列カラム\"\"\",axis=1)\n",
    "    \n",
    "X=train_data.values\n",
    "Y=train_label.values\n",
    "\n",
    "wd = WordDividor()\n",
    "\n",
    "clf = Pipeline([\n",
    "        ('features',FeatureUnion([\n",
    "            ('title',Pipeline([\n",
    "                ('content',TextExtractor()),\n",
    "                ('count_vector',CountVectorizer(analyzer=wd.extract_words)),\n",
    "                ('tfidf',TfidfTransformer())\n",
    "            ])),\n",
    "            ('content',Pipeline([\n",
    "                ('content',TextExtractorone()),\n",
    "                ('count_vector',CountVectorizer(analyzer=wd.extract_words)),\n",
    "                ('tfidf',TfidfTransformer())\n",
    "            ])),\n",
    "            ('day',Pipeline([\n",
    "                ('content',TextExtractortwo()),\n",
    "                ('count_vector',CountVectorizer(analyzer=wd.extract_words)),\n",
    "                ('tfidf',TfidfTransformer())\n",
    "            ])),\n",
    "        ])),\n",
    "        #('classifier',SGDClassifier(loss='hinge',random_state=42))\n",
    "        #('classifier',RandomForestClassifier(n_estimators=100,random_state=42))\n",
    "        ('classifier',LogisticRegression(class_weight = \"balanced\",random_state=42))\n",
    "    ])\n",
    "   \n",
    "    \n",
    "clf.fit(X,Y)\n",
    "    \n",
    "print(\"feature_union\")\n",
    "    \n",
    "includeday=clf.predict(test_data.iloc[:,0:].values)\n",
    "print(includeday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submid = pd.DataFrame({'id': test[\"\"\"特定の列カラム\"\"\"]})\n",
    "submission = pd.concat([submid, pd.DataFrame(includeday, columns = [\"\"\"特定の列カラム\"\"\"])], axis=1)\n",
    "submission.to_csv('answer.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
